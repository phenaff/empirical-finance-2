<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2 The Problem of Induction | Odds &amp; Ends" />
<meta property="og:type" content="book" />

<meta property="og:image" content="/img/social_image.png" />
<meta property="og:description" content="An open access textbook for introductory philosophy courses on probability and inductive logic." />
<meta name="github-repo" content="jweisber/vip" />

<meta name="author" content="Jonathan Weisberg" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="An open access textbook for introductory philosophy courses on probability and inductive logic.">

<title>2 The Problem of Induction | Odds &amp; Ends</title>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@jweisber" />
<meta name="twitter:creator" content="@jweisber" />
<meta name="twitter:title" content="2 The Problem of Induction | Odds &amp; Ends" />
<meta name="twitter:description" content="An open access textbook for introductory philosophy courses on probability and inductive logic." />
<meta name="twitter:image" content="img/social_image.png" />


<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["output/HTML-CSS"],
  "HTML-CSS": {
    availableFonts: ["Gyre-Pagella"],
    preferredFont: "Gyre-Pagella",
    webFont: "Gyre-Pagella",
    imageFont: "Gyre-Pagella"
  }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      extensions: ["color.js"]
    }
  });
</script>





<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="custom.css" type="text/css" />

</head>

<body>


<div style="display: none;">
$$
  \newcommand{\given}{\mid}
  \renewcommand{\neg}{\mathbin{\sim}}
  \renewcommand{\wedge}{\mathbin{\&}}
  \newcommand{\p}{Pr}
  \newcommand{\degr}{^{\circ}}
  \newcommand{\E}{E}
  \newcommand{\EU}{EU}
  \newcommand{\u}{U}
  \newcommand{\pr}{Pr}
  \newcommand{\po}{Pr^*}
  \definecolor{bookred}{RGB}{228,6,19}
  \definecolor{bookblue}{RGB}{0,92,169}
  \definecolor{bookpurple}{RGB}{114,49,94} 
$$
</div>

<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li class="part"><span><b>Part I</b></span></li>
<li><a href="the-monty-hall-problem.html#the-monty-hall-problem"><span class="toc-section-number">1</span> The Monty Hall Problem</a>
<ul>
<li><a href="the-monty-hall-problem.html#diagramming-the-solution"><span class="toc-section-number">1.1</span> Diagramming the Solution</a></li>
<li><a href="the-monty-hall-problem.html#lessons"><span class="toc-section-number">1.2</span> Lessons Learned</a></li>
<li><a href="the-monty-hall-problem.html#exercises">Exercises</a></li>
</ul></li>
<li><a href="the-problem-of-induction.html#the-problem-of-induction"><span class="toc-section-number">2</span> The Problem of Induction</a>
<ul>
<li><a href="the-problem-of-induction.html#the-dilemma">The Dilemma</a></li>
<li><a href="the-problem-of-induction.html#the-problem-of-induction-vs.-the-grue-paradox">The Problem of Induction vs. the Grue Paradox</a></li>
<li><a href="the-problem-of-induction.html#probability-theory-to-the-rescue">Probability Theory to the Rescue?</a></li>
<li><a href="the-problem-of-induction.html#exercises-1">Exercises</a></li>
</ul></li>
<li><a href="solutions-to-selected-exercises.html#solutions-to-selected-exercises"><span class="toc-section-number">3</span> Solutions to Selected Exercises</a>
<ul>
<li><a href="solutions-to-selected-exercises.html#chapter-1">Chapter 1</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-2">Chapter 2</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-3">Chapter 3</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-4">Chapter 4</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-5">Chapter 5</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-6">Chapter 6</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-7">Chapter 7</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-8">Chapter 8</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-9">Chapter 9</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-11">Chapter 11</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-12">Chapter 12</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-13">Chapter 13</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-14">Chapter 14</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-16">Chapter 16</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-17">Chapter 17</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-18">Chapter 18</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-19">Chapter 19</a></li>
<li><a href="solutions-to-selected-exercises.html#chapter-20">Chapter 20</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="the-problem-of-induction" class="section level1" number="2">
<h1><span class="header-section-number">2</span> The Problem of Induction</h1>
<div class="epigraph">
<p>
It’s tough to make predictions, especially about the future.<br />
—Yogi Berra
</p>
</div>
<p><span class="newthought">Many</span> inductive arguments work by projecting an observed pattern onto as-yet unobserved instances. All the ravens we’ve observed have been black, so all ravens are. All the emeralds we’ve seen have been green, so all emeralds are.</p>
<p>The assumption that the unobserved will resemble the observed seems to be central to induction. Philosophers call this assumption the <em>Principle of Induction</em>.<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> See <a href="#indargs">Section</a> <a href="#indargs"><strong>??</strong></a> and <a href="#grue">Appendix</a> <a href="#grue"><strong>??</strong></a> for previous discussions of the Principle of Induction.</span> But what justfies this assumption? Do we have any reason to think the parts of reality we’ve observed so far are a good representation of the parts we haven’t seen yet?</p>
<p>Actually there are strong reasons to doubt whether this assumption can be justified. It may be impossible to give any good argument for expecting the unobserved to resemble the observed.</p>
<div id="the-dilemma" class="section level2 unnumbered">
<h2>The Dilemma</h2>
<p>We noted in [Chapter 2][Logic] that there are two kinds of argument, inductive and deductive. Some arguments establish their conclusions necessarily, others only support them with high probability. If there is an argument for the Principle of Induction, it must be one of these two kinds. Let’s consider each in turn.</p>
<p>Could we give an inductive argument for the Principle of Induction? At first it seems we could. Scientists have been using inductive reasoning for millenia, often with great success. Indeed, it seems humans, and other creatures too, have relied on it for much longer, and could not have survived without it. So the Principle of Induction has a very strong track record. Isn’t that a good argument for believing it’s correct?</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="img/hume.png" alt="David Hume (1711--1776) raised the problem of induction in $1739$. Our presentation of it here is somewhat modernized from his original argument." width="200"  />
<!--
<p class="caption marginnote">-->Figure 2.1: David Hume (1711–1776) raised the problem of induction in <span class="math inline">\(1739\)</span>. Our presentation of it here is somewhat modernized from his original argument.<!--</p>-->
<!--</div>--></span>
</p>
<p>No, because the argument is circular. It uses the Principle of Induction to justify believing in the Principle of Induction. Consider that the argument we are attempting looks like this:</p>
<div class="argument">
<p>
The principle has worked well when we’ve used it in the past.<br />
Therefore it will work well in future instances.
</p>
</div>
<p>This is an inductive argument, an argument from observed instances to ones as yet unobserved. So, under the hood, it appeals to the Principle of Induction. But that’s exactly the conclusion we’re trying to establish. And one can’t use a principle to justify itself.</p>
<p>What about our second option: could a deductive argument establish the Principle of Induction? Well, by definition, a deductive argument establishes its conclusion with necessity. Is it necessary that the unobserved will be like the observed? It doesn’t look like it. It seems perfectly possible that tomorrow the world will go haywire, randomly switching from pattern to pattern, or even to no pattern at all.</p>
<p>Maybe tomorrow the sun will fail to rise. Maybe gravity will push apart instead of pull together, and all the other laws of physics will reverse too. And just as soon as we get used to those patterns and start expecting them to continue, another pattern will arise. And then another. And then, just as we give up and come to have no expectation at all about what will come next, everything will return to normal. Until we get comfortable and everything changes again.</p>
<p>Thankfully, our universe hasn’t been so mischievous. We get surprised now and again, but for the most part inductive reasoning is pretty reliable, when we do it carefully. But we’re lucky in this respect, is the point.</p>
<p>Nature <em>could</em> have been mischievous, totally unpredictable. It is not a necessary truth that the unobserved must resemble the observed. And so it seems there cannot be a deductive argument for the Principle of Induction. Because such an argument would establish the principle as a necessary truth.</p>
</div>
<div id="the-problem-of-induction-vs.-the-grue-paradox" class="section level2 unnumbered">
<h2>The Problem of Induction vs. the Grue Paradox</h2>
<p>If you read <a href="#grue">Appendix</a> <a href="#grue"><strong>??</strong></a>, you know of another famous problem with the Principle of Induction: the grue paradox. (If you haven’t read that chapter, you might want to skip this section.)</p>
<p>The two problems are quite different, but it’s easy to get them confused. The problem we’re discussing here is about justifying the Principle of Induction. Is there any reason to believe it’s true? Whereas the grue paradox points out that we don’t even really know what the principle says, in a way. It says that what we’ve observed is a good indicator of what we haven’t yet obsered. But in what respects? Will unobserved emeralds be green, or will they be grue?</p>
<p>So the challenge posed by grue is to spell out, precisely, what the Principle of Induction says. But even if we can meet that challenge, this challenge will remain. Why should we believe the principle, once it’s been spelled out? Neither a deductive argument nor an inductive argument seems possible.</p>
</div>
<div id="probability-theory-to-the-rescue" class="section level2 unnumbered">
<h2>Probability Theory to the Rescue?</h2>
<p>The Problem of Induction is centuries old. Isn’t it out of date? Hasn’t the modern, mathematical theory of probability solved the problem for us?</p>
<p>Not at all, unfortunately. One thing we learn in this book is that the laws of probability are very weak in a way. They don’t tell us much, without us first telling them what the prior probabilities are. And as we’ve seen over and again throughout Part III, <a href="#priors">the problem of priors</a> is very much unsolved.</p>
<p>For example, suppose we’re going to flip a mystery coin three times. We don’t know whether the coin is fair or biased, but we hope to have some idea after a few flips.</p>
<p>Now suppose we’ve done the first two flips, both heads. The Principle of Induction says we should expect the next flip to be heads too. At least, that outcome should now be more probable.</p>
<p>Do the laws of probability agree? Well, we need to calculate the quantity:
<span class="math display">\[ \p(H_3 \given H_1 \wedge H_2).\]</span>
The definition of conditional probability tells us:
<span class="math display">\[
  \begin{aligned}
    \p(H_3 \given H_2 \wedge H_1)
      &amp;= \frac{\p(H_3 \wedge H_2 \wedge H_1)}{\p(H_2 \wedge H_1)}.
  \end{aligned}
\]</span>
But the laws of probability don’t tell us what numbers go in the numerator and the denominator.</p>
<p>The numbers have to be between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. And we have to be sure mutually exclusive propositions have probabilities that add up, according to the Additivity rule. But that still leaves things wide open.</p>
<p>For example, we could assume that all possible sequences of heads and tails are equally likely. In other words:
<span class="math display">\[ \p(HHH) = \p(THH) = \p(HTH) = \ldots = \p(TTT) = 1/8. \]</span>
Given that assumption, we get the result that <span class="math inline">\(\p(H_3 \given H_2 \wedge H_1) = 1/2\)</span>.
<span class="math display">\[
  \begin{aligned}
    \p(H_3 \given H_2 \wedge H_1)
      &amp;= \frac{\p(H_3 \wedge H_2 \wedge H_1)}{\p(H_2 \wedge H_1)}\\
      &amp;= \frac{1/8}{1/8 + 1/8}\\
      &amp;= 1/2.
  \end{aligned}
\]</span>
But that means the first two flips didn’t tell us anything about the third! Even after we got two heads, the chance of another heads is still stuck at <span class="math inline">\(1/2\)</span>, same as it was to start with.</p>
<p><span class="newthought">We didn’t</span> <em>have</em> to assume all possible sequences are equally likely. We can make a different assumption, and get a different result.</p>
<p>Let’s try assuming instead that all possible <em>frequencies</em> of heads are equally probable. In other words, the probability of getting <span class="math inline">\(0\)</span> heads is the same as the probability of getting <span class="math inline">\(1\)</span> head, which is also the same as the probability of getting <span class="math inline">\(2\)</span> heads, and likewise for <span class="math inline">\(3\)</span> heads. So we’re grouping the possible sequences like so:</p>
<ul>
<li><span class="math inline">\(0\)</span> heads: <span class="math inline">\(TTT\)</span></li>
<li><span class="math inline">\(1\)</span> head: <span class="math inline">\(HTT\)</span>, <span class="math inline">\(THT\)</span>, <span class="math inline">\(TTH\)</span></li>
<li><span class="math inline">\(2\)</span> heads: <span class="math inline">\(HHT\)</span>, <span class="math inline">\(HTH\)</span>, <span class="math inline">\(THH\)</span></li>
<li><span class="math inline">\(3\)</span> heads: <span class="math inline">\(HHH\)</span></li>
</ul>
<p>Each grouping has the same probability, <span class="math inline">\(1/4\)</span>. And for the groups in the middle, which have multiple members, we divide that evenly between the members. So <span class="math inline">\(\p(HTH) = 1/12\)</span>, for example, but <span class="math inline">\(\p(TTT) = 1/4\)</span>.</p>
<p>This might seem a funny way of assigning prior probabilities. But it actually leads to very sensible results; the same results as the <a href="#succession">Rule of Succession</a> in fact! For example, we get <span class="math inline">\(\p(H_3 \given H_2 \wedge H_1) = 3/4\)</span>:
<span class="math display">\[
  \begin{aligned}
    \p(H_3 \given H_1 \wedge H_2)
      &amp;= \frac{\p(H_3 \wedge H_2 \wedge H_1)}{\p(H_2 \wedge H_1)}\\
      &amp;= \frac{1/4}{1/4 + 1/12}\\
      &amp;= 3/4.
  \end{aligned}
\]</span>
So, on this analysis, the first two tosses do tell us what to expect on the next toss.</p>
<p><span class="newthought">We’ve</span> seen two different ways of assigning prior probabilities, which lead to very different results. The first way, where we assume all possible sequences are equally likely, disagrees with the Principle of Induction. Our observations of the first two flips tell us nothing about the next one. But the second way, where we assume all possible frequencies are equally likely, agrees with with the Principle of Induction. Observing heads on the first two does tell us to expect another heads on the next one.</p>
<p>Both assumptions are consistent with the laws of probability. So those laws don’t, by themselves, tell us what to expect. The laws of probability only tell us what to expect once we’ve specified the prior probabilities. The problem of induction challenges us to justify one choice of prior probabilities over the alternatives.</p>
<p>In the <span class="math inline">\(280\)</span> years since this challenge was first raised by David Hume, no answer has gained general acceptance.</p>
</div>
<div id="exercises-1" class="section level2 unnumbered">
<h2>Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Suppose we do <span class="math inline">\(100\)</span> draws, with replacement, from an urn containing an unknown mixture of black and white balls. All <span class="math inline">\(100\)</span> draws come out black. Which of the following is correct?</p>
<ol style="list-style-type: lower-alpha">
<li>According to the laws of probability, the next draw is more likely to be black than white.</li>
<li>According to the laws of probability, the next draw is more likely to be white than black.</li>
<li>According to the laws of probability, the next draw is equally likely to be black vs. white.</li>
<li>The laws of probability are consistent with any of the above conclusions; it depends on the prior probabilities.</li>
<li>None of the above.</li>
</ol></li>
<li><p>Write a short essay (<span class="math inline">\(3\)</span>–<span class="math inline">\(4\)</span> paragraphs) explaining the problem of induction. Your essay should include all of the following:</p>
<ul>
<li>a clear, accurate explanation of the Principle of Induction,</li>
<li>a clear, accurate explanation of the dilemma we face in justifying the Principle of Induction,</li>
<li>a clear, accurate explanation of the challenge for the deductive horn of this dilemma, and</li>
<li>a clear, accurate explanation of the challenge for the inductive horn of the dilemma.</li>
</ul></li>
<li><p>A coin will be tossed <span class="math inline">\(3\)</span> times, so the number of heads could be <span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, or <span class="math inline">\(3\)</span>. Suppose all <span class="math inline">\(4\)</span> of these possibilities are equally likely. Moreover, any two sequences with the same number of heads are also equally likely. For example, <span class="math inline">\(\p(H_1 \wedge H_2 \wedge T_3) = \p(T_1 \wedge H_2 \wedge H_3)\)</span>. Answer each of the following.</p>
<ol style="list-style-type: lower-alpha">
<li>What is <span class="math inline">\(\p(H_2 \given H_1)\)</span>?</li>
<li>What is <span class="math inline">\(\p(H_3 \given H_1 \wedge H_2)\)</span>?</li>
</ol>
<p>Now suppose we do <span class="math inline">\(4\)</span> tosses instead of <span class="math inline">\(3\)</span>. The prior probabilities follow the same rules: all possible numbers of heads are equally likely, and any two sequences with the same number of heads are equally likely.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>If the first <span class="math inline">\(n\)</span> tosses come up heads, what is the probability the next toss will come up heads? In other words, give a formula for <span class="math inline">\(\p(H_{n+1} \given H_1 \wedge \ldots \wedge H_n)\)</span> in terms of <span class="math inline">\(n\)</span>.</li>
<li>What if only <span class="math inline">\(k\)</span> out of the first <span class="math inline">\(n\)</span> tosses land heads, then what formula gives the probability of heads on the next toss?</li>
</ol></li>
<li><p>Suppose a computer program prints out a stream of A’s and B’s. After observing the sequence A, A, B, we want to know the probability of an A next.</p>
<p>Our friend Charlie suggests we reason as follows. We are going to observe <span class="math inline">\(4\)</span> characters total. Before we observed any characters, there were <span class="math inline">\(5\)</span> possibilities: the total number of As could turn out to be <span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(3\)</span>, or <span class="math inline">\(4\)</span>. And all of these possibilities are equally likely. So each has prior probability <span class="math inline">\(1/5\)</span>.</p>
<p>Some of these possibilities can be subdivided. For example, there are <span class="math inline">\(4\)</span> ways to get <span class="math inline">\(3\)</span> A’s:</p>
<p>A, A, A, B<br />
A, A, B, A<br />
A, B, A, A<br />
B, A, A, A</p>
<p>So each of these sequences gets <span class="math inline">\(1/4\)</span> of <span class="math inline">\(1/5\)</span>, in other words prior probability <span class="math inline">\(1/20\)</span>.</p>
<p>According to Charlie’s way of reasoning, what is the probability the <span class="math inline">\(4\)</span>th character will be an A, given that the first <span class="math inline">\(3\)</span> were A, A, B?</p></li>
<li><p>In this chapter we considered a coin to be flipped <span class="math inline">\(3\)</span> times, with the same prior probability for every possible sequence of heads/tails. Now suppose the coin will be flipped some very large number of times, <span class="math inline">\(n\)</span>. And again suppose the prior probability is the same for every possible sequence of heads/tails. Prove that no matter how many times the coin lands heads, the probability of heads on the next toss is still <span class="math inline">\(1/2\)</span>. In other words, prove that <span class="math inline">\(\p(H_{k+1} \given H_1 \wedge \ldots \wedge H_{k}) = 1/2\)</span> no matter how large <span class="math inline">\(k\)</span> gets.</p></li>
<li><p>Suppose a coin will be flipped <span class="math inline">\(3\)</span> times. There are <span class="math inline">\(2^3 = 8\)</span> possible sequences of heads/tails that we might get. Find a way of assigning prior probabilities to these <span class="math inline">\(8\)</span> sequences so that, the more heads we observe, the <em>less</em> likely it becomes we’ll get heads on the next toss. In other words, assign a prior probability to each sequence so that <span class="math inline">\(\p(H_2 \given H_1) &lt; \p(H_2)\)</span> and <span class="math inline">\(\p(H_3 \given H_2 \wedge H_1) &lt; \p(H_3 \given H_1)\)</span>.</p></li>
<li><p>Suppose a coin will be flipped <span class="math inline">\(4\)</span> times. Recall the two different ways of assigning a probability to each possible sequence of heads and tails discussed in the chapter:</p>
<ul>
<li>Scheme 1: all possible sequences have the same probability.</li>
<li>Scheme 2: all possible <em>frequencies</em> (numbers of heads) have the same probability, and all sequences that share the same frequency have the same probability.</li>
</ul>
<p>According to each scheme, how probable is each of the following propositions?</p>
<ul>
<li><span class="math inline">\(A =\)</span> All tosses will land the same way.<br />
</li>
<li><span class="math inline">\(B =\)</span> The number of heads and tails will be the same.</li>
</ul>
<p>Now answer the same question with <span class="math inline">\(10\)</span> tosses instead of <span class="math inline">\(4\)</span>.</p></li>
</ol>

</div>
</div>
<p style="text-align: center;">
<a href="the-monty-hall-problem.html"><button class="btn btn-default">Previous</button></a>
<a href="solutions-to-selected-exercises.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
